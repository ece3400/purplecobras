<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>ECE3400 - Team 2: Purple Cobras</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/agency.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Purple Cobras</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#labs">Labs</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Milestones</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#links">Links</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#about">About</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container">
        <div class="intro-text">
          <div class="intro-lead-in">Welcome!</div>
          <div class="intro-heading text-uppercase">Check Out Our Robot!</div>
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="#labs">Tell Me More!</a>
        </div>
      </div>
    </header>

    <!-- Labs -->
    <section id="labs">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Labs</h2>
            <h3 class="section-subheading text-muted">See our progress!</h3>
          </div>
        </div>
        <div class="row text-center">
          <div class="col-md-3 portfolio-item">
              <a class="portfolio-link" data-toggle="modal" href="#labsModal1">
              <span class="fa-stack fa-4x">
                <i class="fas fa-circle fa-stack-2x text-primary"></i>
                <i class="fas fa-laptop-code fa-stack-1x fa-inverse"></i>
              </span>
              </a>
            <div class="portfolio-caption">
              <h4 class="service-heading">Lab 1</h4>
              <p class="text-muted">Microcontroller</p>
            </div>
          </div>

	  <div class="col-md-3 portfolio-item">
              <a class="portfolio-link" data-toggle="modal" href="#labsModal2">
              <span class="fa-stack fa-4x">
                <i class="fas fa-circle fa-stack-2x text-primary"></i>
                <i class="fas fa-laptop-code fa-stack-1x fa-inverse"></i>
              </span>
              </a>
            <div class="portfolio-caption">
              <h4 class="service-heading">Lab 2</h4>
              <p class="text-muted">Analog Circuitry and FFTs</p>
            </div>
          </div>
          <div class="col-md-3 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#labsModal3">
              <span class="fa-stack fa-4x">
                <i class="fas fa-circle fa-stack-2x text-primary"></i>
                <i class="fas fa-glasses fa-stack-1x fa-inverse"></i>
              </span>
            </a>
            <h4 class="service-heading">Lab 3</h4>
            <p class="text-muted">System Integration and Radio Communication</p>
          </div>
          <div class="col-md-3 portfolio-item">
	    <a class="portfolio-link" data-toggle="modal" href="#labsModal4">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-robot fa-stack-1x fa-inverse"></i>
            </span>
	    </a>
            <h4 class="service-heading">Lab 4</h4>
            <p class="text-muted">FPGA and Shape Detection</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Milestones Grid -->
    <section class="bg-light" id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Milestones</h2>
            <h3 class="section-subheading text-muted">See more progress!</h3>
          </div>
        </div>
        <div class="row">
          <div class="col-md-6 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#portfolioModal1">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/milestone1/thumbnail.jpg" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Milestone 1</h4>
              <p class="text-muted">Line Following</p>
            </div>
          </div>
          <div class="col-md-6 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#portfolioModal2">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/milestone2bot.PNG" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Milestone 2</h4>
              <p class="text-muted">Wall and Robot Detection</p>
            </div>
          </div>
          <div class="col-md-6 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#portfolioModal3">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/milestone3/IMG_2016.PNG" alt="">
            </a>
            <div class="portfolio-caption">
              <h4>Milestone 3</h4>
              <p class="text-muted">DFS</p>
            </div>
          </div>
          <div class="col-md-6 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#labsModal4">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/milestone4/colorbar.png" alt="" >
            </a>
		  
		<!-- You will not be able to see this text.   
		  
		  <a class="portfolio-link" data-toggle="modal" href="#labsModal4">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-robot fa-stack-1x fa-inverse"></i>
            </span>
	    </a>
            <h4 class="service-heading">Lab 4</h4>

		  -->
		  
		  
            <div class="portfolio-caption">
              <h4>Milestone 4</h4>
              <p class="text-muted">Image Recognition</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Links -->
    <section id="links">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Links</h2>
            <h3 class="section-subheading text-muted">Explore!</h3>
          </div>
        </div>
        <div class="row text-center">

          <div class="col-md-4">
            <span class="fa-stack fa-4x">
              <a target= "_blank" href=https://docs.google.com/document/d/1-iWddSh8Znvbeo26n1ewbqpnlS4QQ3WxkqLcQgeoDS4/edit>
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-file-signature fa-stack-1x fa-inverse"></i>
              </a>
            </span>
            <h4 class="service-heading">Contract</h4>
            <p class="text-muted">See how our team functions!</p>
          </div>
          <div class="col-md-4">
            <span class="fa-stack fa-4x">
	      <a target= "_blank" href=https://docs.google.com/document/d/1tK3uGPCvymwRQQyy8TZ3YWboe7zBefwLGgEb4atzO8A/edit>
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-glasses fa-stack-1x fa-inverse"></i>
	      </a>
            </span>
            <h4 class="service-heading">Ethics</h4>
            <p class="text-muted">Our Ethics Policy</p>
	</div>
		<div class="col-md-3 portfolio-item">
	    <a class="portfolio-link" data-toggle="modal" href="#finalproj">
            <span class="fa-stack fa-4x">
              <i class="fas fa-circle fa-stack-2x text-primary"></i>
              <i class="fas fa-robot fa-stack-1x fa-inverse"></i>
            </span>
	    </a>
            <h4 class="service-heading">Competition and Beyond</h4>
            <p class="text-muted">Final Robot Design</p>
          </div>
	  
      </div>
    </section>

    <!-- About -->
    <section class="bg-light" id="about">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Our Amazing Team</h2>
            <h3 class="section-subheading text-muted">Who are we?</h3>
          </div>
        </div>
        <div class="row">
          <div class="col-sm-3">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/team/1.jpg" alt="">
              <h4>April Chen</h4>
              <p class="text-muted">Junior CS Major</p>
            </div>
          </div>
          <div class="col-sm-3">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/team/2.jpg" alt="">
              <h4>Brian Dempsey</h4>
              <p class="text-muted">Junior ECE Major</p>
            </div>
          </div>
          <div class="col-sm-3">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/team/3.jpg" alt="">
              <h4>Elliot Sotnick</h4>
              <p class="text-muted">Junior ECE Major</p>
            </div>
          </div>
          <div class="col-sm-3">
            <div class="team-member">
              <img class="mx-auto rounded-circle" src="img/team/3.jpg" alt="">
              <h4>Jaylen Keith</h4>
              <p class="text-muted">Junior ECE Major</p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-md-4">
            <span class="copyright">ECE3400 Team 2: Purple Cobras</span>
          </div>
          <div class="col-md-4">
            <ul class="list-inline social-buttons">
              <li class="list-inline-item">
                <a target="_blank" href=https://github.com/ece3400/purplecobras>
                  <i class="fab fa-github"></i>
                </a>
              </li>
            </ul>
          </div>
          <div class="col-md-4">
            <ul class="list-inline quicklinks">
              <li class="list-inline-item">
                <a target="_blank" href=https://cei-lab.github.io/ece3400-2018/>ECE 3400</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </footer>

    <!-- Labs Modals -->

    <!-- Modal 1 -->
    <div class="portfolio-modal modal fade" id="labsModal1" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h2 class="text-uppercase">Lab 1: Microcontroller</h2>
                  <p class="item-intro text-muted">Objective: This lab’s objective is to learn how to program the Arduino Uno through Arduino IDE and
			  also learn about its functionalities. We wrote simple programs for the Arduino and then assembled our robot and programmed
			  it to perform basic autonomous driving.
		  </p>
                  <p> <h5>Teams:</h5>
                    <ol class="list-inline">
                      <li>Brian Dempsey and Jaylen Keith</li>
                      <li>April Chen and Elliot Sotnick</li>
                    </ol>
                    Teams eventually merged due to issues with getting IDE to function on some of our laptops. Both teams performed all parts of the lab involving testing the Arduino separately before building and programming the robot together.
                  </p>
                  <p> <h5>Materials:</h5>
                    <ul class="list-inline">
                      <li>1 Arduino Uno (in the box)</li>
                      <li>1 USB A/B cable (in the box)</li>
                      <li> 2 Continuous rotation servos</li>
                      <li> 1 LED </li>
                      <li> 1 Potentiometer</li>
                      <li> 1 Solderless breadboard</li>
                      <li> 1 Line sensor</li>
                      <li> Various robot pieces</li>
                      <li> Various resistors (kΩ range)</li>
                    </ul>
                  </p>
                  <p> <h5>Procedure:</h5>
                    <ol class="list-inline">
                      <li>1. Test the internal Blink code</li>
                      <li>2. Blink an external LED</li>
                      <li>3. Read the value of a potentiometer </li>
                      <li>4. Control a LED with the potentiometer</li>
                      <li>5. Control a servo with the potentiometer</li>
                      <li>6. Build a robot and code it to drive autonomously</li>
                    </ol>
                  </p>
                  <p> <h6>1. Test the internal Blink code</h6>
                    We compiled the code for the Blink function and programmed our Arduino with it to test the connections
                    of the Arduino Uno to IDE. The code for this was given to us in Arduino IDE.
                    The program worked and the LED on the Arduino blinked on and off as it should.
                  </p>
                  <p> <h6>2. Blink an external LED</h6>
                    We modified the Blink code to blink an external LED connected to a digital pin on the Arduino.
                    The LED was in series with a 330Ω resistor. The LED was successfully made to blink,
                    and our code for this program is below.
                    <img class="img-fluid d-block mx-auto" src="img/lab1/blink_modified.png" alt="">
                  </p>
                  <p> <h6>3. Read the value of a potentiometer </h6>
                    We connected a potentiometer in series with a 330Ω resistor as an input to the Arduino,
                    and programmed the Arduino to read the voltage with analogRead and print it to the screen every half-second.
                    The Arduino was able to do this as we changed the value of the voltage by tuning the potentiometer.
                  </p>
                  <p> <h6>4. Control an LED with the potentiometer</h6>
                    Once we were able to read in the potentiometer value,
                    we wired an LED in series with a 330Ω resistor and a digital out pin.
                    Using pulse width modulation from the digital out,
                    we were able to control the brightness of the LED.
                    The map method was helpful to map servo inputs to PWM duty cycle.
                  <!-- This is a comment -->
                  </p>
                  <p> <h6>5. Control a servo with the potentiometer</h6>
                    Because Arduino has a built-in Servo library,
                    controlling the servo was not much harder than controlling the LED.
                    Again, we used map to map the input from the potentiometer to an input to the Servo.write function.
                    <img class="img-fluid d-block mx-auto" src="img/lab1/servo.png" alt="">
                  </p>
                  <p> <h6>6. Build a robot and code it to drive autonomously</h6>
                    Now that we were done with the lab tasks, we started building our robot.
                    The robot was built with one wheel (each with its own servo) on each side and one stabilization bearing in the front.
                    After using the oscilloscope to measure the line sensor outputs above each color tile,
                    we wrote stops, turns, or drives the robot depending on the line sensor reading.
                    Although the robot did not drive perfectly by the time we left lab,
                    it was a good starting point that we will be building off of.
                    <img class="img-fluid d-block mx-auto" src="img/lab1/autonomous.png" alt="">
                  </p>
                  <p>
                    <div class="embed-responsive embed-responsive-16by9">
                      <iframe width="560" height="315" src="https://www.youtube.com/embed/Q3JCB6bpPFQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                  </p>
		<p> <h5> Conclusion </h5>
		The first part of this lab was fairly simple, and we were able to get the LED to blink and control it with a potentiometer using the Arduino without much trouble. We were able to get our robot built and get line sensor readings for the lines. We were not able to get the robot turning properly based on the line sensor readings yet, but it was able to travel in a square using timing on its turns. This was a good start to build off, and next we have to get the robot turning based on the sensor readings.
		</p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 2 -->
    <div class="portfolio-modal modal fade" id="labsModal2" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h2 class="text-uppercase">Lab 2: Analog Circuitry and FFTs</h2>
                  <p class="item-intro text-muted">Objective: In this lab we added a microphone and an IR sensor to our robot. In order to accomplish this, we added a digital filter and analog filters so that the microphone could detect 660 Hz and the IR sensor could distinguish between other robots and decoys.</p>
                  <p> <h5>Teams:</h5>
                    <ol class="list-inline">
                      <li>Microphone/Acoustic - Brian and Elliot</li>
                      <li>IR/Optical - April and Jaylen</li>
                    </ol>
                  </p>
                  <p> <h5>Acoustic Team Materials</h5>
                    <ul class="list-inline">
	       	      <li> 1 Arduino Uno </li>
		      <li> 1 breadboard </li>
		      <li> 1 microphone </li>
	  	      <li> 1 LM358 op amp </li>
		      <li> Various resistors and capacitors </li>
                    </ul>
                  </p>
                  <p> <h5>Procedure:</h5>
                    <ol class="list-inline">
                      <li>1. Observe raw microphone output </li>
                      <li>2. Filter and/or amplify the signal and observe new output </li>
                      <li>3. Utilize Open Music Labs FFT library to detect 660 Hz signal on Arduino </li>
                    </ol>
                  </p>
                  <p> <h6>1. Observe Raw Microphone Output</h6>
                    After generating a 660 Hz signal, we observed the microphone output on the oscilloscope
		  in the time and frequency domain. The microphone’s AC amplitude was in the 1 mV range, well
		  below the accuracy of the Arduino. We determined that an active filter would properly
		  condition the microphone signal.
                  </p>
                  <p> <h6>2. Filter and/or amplify the signal and observe new output </h6>
                    The following circuit was constructed, with cutoff frequencies centered around 660 Hz:
                    <img class="img-fluid d-block mx-auto" src="img/lab2/image9.png" alt="">
		  </p>
	          <p>
		    Below is a picture of the circuit after we built it on the breadboard.
		    <img class="img-fluid d-block mx-auto" src="img/lab2/image6.jpg" height="10" alt="">
                  </p>
	      	  <p>
		    Since the open loop gain of the op amp is around 100, we determined that a gain of around
		    ~10 would be easy to implement, stable, and large enough for the Arduino to read. A scope
			  FFT pre-amplification and post-amplification are shown below.
	      	  </p>
	      	  <p>
		   <img class="img-fluid d-block mx-auto" src="img/lab2/image2.jpg" alt="">
	         </p>
	      <p>
		   <img class="img-fluid d-block mx-auto" src="img/lab2/image4.jpg" alt="">
	      </p>
                  <p> <h6>3. Utilize Open Music Labs FFT library to detect 660 Hz signal on Arduino </h6>
                    As recommended, the output was then sent to an analog pin on the Arduino with a series
	      resistor preventing overcurrent. We then used the Open Music Labs FFT to analyze the incoming
	      signal in the frequency domain and figure out which bin represents the 660Hz signal.
		<p> <h5>Optical Team</h5>
                  <p> <h6>Objective</h6>
                   Capture inputs from an IR sensor to detect nearby robots emitting IR at 6.08kHz, and distinguish
	      them from decoys emitting at 18kHz
                  </p>
                  <p> <h6>Optical Team Materials</h6>
                      <ul class="list-inline">
                      <li> 1 Arduino Uno</li>
	       	      <li> 1 IR Phototransistor </li>
		      <li> 1 IR Hat </li>
		      <li> 1 IR Decoy </li>
		      <li> Various resistors and capacitors </li>
                    </ul>
                  </p>
		<p> <h6>Procedure</h6>
		      <ul class="list-inline">
                      <li> 1. Test the FFT code</li>
	       	      <li> 2. Test the IR sensor with the IR hat</li>
		      <li> 3. Design and test additional circuitry as needed for the Arduino </li>
		      <li> 4. Connect the circuit to the Arduino and make it detect the presence of an IR hat </li>
		      <li> 5. Differentiate between an IR hat and an IR decoy </li>
                    </ul>
		</p>

		<p> <h6>1. FFT</h6>
			We decided to use the Analog to Digital Converter (ADC) over AnalogRead() because AnalogRead()
has a maximum reading rate of ~9kHZ, which is too slow for our purposes. We want to be able to sample at least double the
speed of our frequency giving us 12.16kHz. The fft library from Open Music Labs provided the following fft algorithm to run
the arduino using the ADC instead of AnalogRead().
		<img class="img-fluid d-block mx-auto" src="img/lab2/image1.png" alt="">
		</p>
		<p>
			We first did a unit test to test that this code was working. Using the function generator, we plotted
			the data from the serial monitor using Excel to create a graph of the FFTs of the signals that we care
			about: 6.08kHz and 18kHz. The line:
			ADCSRA = 0xe5
			Changes the prescaler from 128 to 32, this enables us to run the ADC clock at 500kHz. We can then calculate
			the width of each bin to be (16MHz/32 prescaler) / 13 clock cycles  = ~38kHz. And 38 kHz / 256 bins = ~148.4
			Hz per bin. This means that we would ideally see the peak for 6.08kHz to be in bin 41, and the peak for 18kHz
			in bin 121 which is what is shown below.
		</p>
		<p>
		<img class="img-fluid d-block mx-auto" src="img/lab2/image8.png" alt="">
		</p>
		<p> <h6>2. Test the IR sensor with the IR hat </h6>
			First we we mounted the IR hat on top of our robot, and powered it with a 9V DC voltage.
			Then we tested the phototransistor alone, wiring it up according to this diagram.
			<img class="img-fluid d-block mx-auto" src="img/lab2/image7.png" alt="">
		</p>
		<p>
			<img class="img-fluid d-block mx-auto" src="img/lab2/image3.png" alt="">
		</p>
		<p>
			We confirmed that the output voltage changed appropriately as the phototransistor was moved closer to the IR hat.
			This is the output when the IR sensor sensed the presence of the IR hat.
			<img class="img-fluid d-block mx-auto" src="img/lab2/image5.jpg" alt="">
		</p>
		<p> <h6>3. Design and test additional circuitry as needed for the Arduino </h6>
			We originally wanted to create a band-pass filter because we wanted to detect 6.08kHz signals but not 18kHz signals.
We also didn’t want to detect lower frequencies that could be from the room lights or sunlight. After we got this to work, we
realized that the signal coming off of the IR hat was too weak to detect from farther distances so we decided that we needed to
amplify the signal. This is so we can detect other robots prior to hitting them. So we then changed our design from a passive
band-pass filter to an active one using an op-amp. Our second design used a band-pass inverting op-amp. However, we ran into
many complications with this design, so we determined that we really only needed the amplifier because we could digitally detect
the signals using the bin outputs, so we ultimately decided to simply use an inverting amplifier.

We chose to have a gain of 10 by TA suggestion. The resistor values that we chose were 100kΩ, and 10kΩ, but the actual values were
98kΩ and 9.88kΩ.

<br/> However, even after doing this, the signal was still not amplified. After checking everything twice, we swapped out the op-amp for a
LF series op-amp and followed Team Alpha's circuit, as shown below.
<img class="img-fluid d-block mx-auto" src="img/lab2/teamalphacircuit.png" alt="">
		</p>
    <p>
      We first checked that the circuit amplified the signal. Below are the before and after amplification photos.
      <img class="img-fluid d-block mx-auto" src="img/lab2/image10.jpg" alt="">
      <img class="img-fluid d-block mx-auto" src="img/lab2/image11.jpg" alt="">
    </p>
    <p>
      Using this circuit, we were able to confirm that our amplifier detected the signal from different distances.
    <p/>
    <p>
      4 inches away
      <img class="img-fluid d-block mx-auto" src="img/lab2/image12.jpg" alt="">
      6 inches away
      <img class="img-fluid d-block mx-auto" src="img/lab2/image13.jpg" alt="">
      8 inches away
      <img class="img-fluid d-block mx-auto" src="img/lab2/image14.jpg" alt="">
    </p>
    <p>
      Looking at the FFT signal, we can see the harmonics as well.
      <img class="img-fluid d-block mx-auto" src="img/lab2/image15.jpg" alt="">
    </p>
		<p> <h6>4. Connect the circuit to the Arduino and make it detect the presence of an IR hat </h6>
			We connected the output of the amplifier to an analog pin on the Arduino and ran our code to make the Arduino detect
the 6.08 kHz signal from the IR hat. We made the Arduino print out “ROBOT” for debugging purposes whenever the bin that 6.08 kHz
was in was over a certain voltage, to show that the Arduino recognized the signal.
		</p>
		<p> <h6>5. Connect the circuit to the Arduino and make it detect the presence of an IR decoy </h6>
			Lastly we tested our setup with the IR hat to make sure that the Arduino received the signal and coded it to not cause
the robot to do anything upon detection.
		</p>
    <p> <h5> Full System </h5>
      To merge all our code together, we decided to simply use the ADC for both the audio and optical.
      Because of this, we were able to use the same code to sample for our signals. To demonstrate that
      we were able to detect them all at the same time, we used Serial prints. This won't be necessary
      for our final prototype and we will remove it to lower the amount of dynamic memory used.
    </p>
    <p>
      By graphing our outputs and comparing them to those we saw before when the code was running each process individually,
      we saw that the signals still peaked in the correct bins even when there are multiple signals being detected.
      <img class="img-fluid d-block mx-auto" src="img/lab2/img16.png" alt="">
      This is why we were still able to use the code below to detect the proper signals.
    </p>
    <p class = text-left>
      <code>
        <br>if (fft_log_out[44] >= 70) { <br>
          Serial.println("ROBOT"); <br>
        }<br>
        if (fft_log_out[121] >= 70) { <br>
          Serial.println("DECOY"); <br>
        } <br>
        if (fft_log_out[6] >= 70) { <br>
          Serial.println("SOUND"); <br>
        }<br>
      </code>
    </p>
		<p> <h5> Conclusion </h5>
		Both teams were able to get their parts working, and our robot now has merged code that works both with audio and IR. The IR team first had trouble getting the analog circuitry to work for the amplifier for the IR phototransistor, but found success after changing the op-amp and using Team Alpha’s design. The phototransistor was able to detect IR signals from the IR hat from a good distance away, but for future improvements we should make sure that it does not detect robots that are too far away.
		</p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 3 -->
    <div class="portfolio-modal modal fade" id="labsModal3" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h2 class="text-uppercase">Lab 3: System Integration and Radio Communication</h2>
                  <p class="item-intro text-muted">Objective: Make the robot start on a 660Hz tone, navigate a maze autonomously, send maze information and update GUI.</p>
                  <p> <h5>Teams:</h5>
                    <ol class="list-inline">
                      <li>Radio - Brian and Jaylen</li>
                      <li>Robot - April and Elliot</li>
                    </ol>
                  </p>
                  <p> <h5> Radio Team </h5> </p>
                  <p> <h6> Objective: Send information the robot discovers about the maze to a base station which will display this info using the GUI. </h6> </p>
                  <p> <h6>Radio Team Materials</h6>
                    <ul class="list-inline">
                      <li> 2 Nordic nRF24L01+ transceivers </li>
                      <li> 2 Arduino Unos </li>
                      <li> 2 radio breakout boards with headers </li>
                    </ul>
                  </p>
                  <p> <h6>Procedure:</h6>
                    <ol class="list-inline">
                      <li>1. Getting Started </li>
                      <li>2. Sending Maze Information Between Arduinos</li>
                      <li>3. Simulating Your Robot</li>
                      <li>4. Base Station-to-GUI Transmission</li>
                    </ol>
                  </p>
                  <p> <h6>1. Getting Started</h6>
                    We started off the radio portion of the lab by choosing the pipes we would be using throughout
	      	    the lab. Being Group 2 and in Monday lab, we used 4 and 5 for our pipes. After this, we
	      		downloaded the getting started sketch and made sure that it was able to run on our Arduinos.
	      		We found that on maximum power, the radios were able to trasmit from over 10 feet away.
	      		Once on maximum power, we found that we never dropped packets.
                  </p>
                  <p> <h6>2. Sending Maze Information Between Arduinos </h6>
                    	At this point, we began modifying the getting started sketch so that it would serve
	    		the purposes we needed it to. One of the changes we made was for the radio to send
	    		unsigned chars rather than ints because we needed to send bits containing our bit masks
	    		to the receiver. After successfully getting the radios to send chars, we started
	    		coming up with the bit masks we would use for relaying information to the ground
	    		station while taking up as little storage as possible. We determined that we would
	    		send robot movement direction information, whether a space is explored or unexpored,
	    		and whether a robot is present in a square all in one byte. This left treasure information
	    		and wall information for the second byte. We decided to send direction information rather
	    		than physical coordinates each time because while it is slightly harder to decipher the
	    		sent information on the receiver end, it means we can send fewer bytes and store less
	    		information. Direction ended up taking 3 bits, one for whether we had moved, and the next
	    		two to store which direction we had moved in if we moved. We got away with using only two
	    		bits for the four directions because we made North 00 (which we could do since we made the
	    		move/no move bit), East 01, South 10, and West 11. The next two bits were a simple yes or no
	    		indication of whether we had explored a space and whether a robot is present or not. For the
	    		second byte, we had the first bit of the treasure information indicate the color of the treasure,
	    		and then the next two bits indicated the shape or if no present was present in a similar method
	    		to that used for our direction bits. Finally, for our walls we knew we could have multiple walls
	    		at each location so we couldn't use only two bits (this would only allow for one wall at each
	    		position). We ended up using four bits where each bit indicated whether there was a north, east,
	    		south, and/or west wall present.
                  </p>
		<p>
			By combining our bits this way, we were able to use only two bytes per transaction between
			sender and receiver. After this was developed, we began working on creating a virtual maze
			and robot which we used to test the functionality of our code, and later on, the functionality
			of the GUI. We set up a simple 9x9 array and used a for loop to have our robot move through the
			array. As it moved, we had simple if conditions to check its location and depending on the location
			the robot was at, we put in different conditions such as that walls were present, or that we wanted
			the robot to change directions. Using the serial monitor, we checked on the receiver side to ensure
			the robot was driving in the directions we thought and that we were detecting the walls and treasures
			that we had set up in the maze.
		</p>
		<p>
			At this point, we were ready to set up the GUI. We started this by taking out every print statement
			in the code, and then adding many switch case statements to our receiver code. By ANDing the received
			bytes with different bits, we could retrieve exactly the bits we wanted to check, and then we would
			shift these bits so that they were the least significant bits. After doing this, it was very easy to
			figure out the possible values we could receive and use the bitmasks we had come up with earlier to
			decode the bits we received. We used Arduino Strings to concatenate the strings we wanted to create
			depending on the conditions in the switch case statements. Finally, after decoding each byte, one at a time,
			we put in a Serial.println() and printed our code to the GUI. Attached below is an example of using a switch
			case statement to determine whether a robot is present in a given square. As can be seen, we and the received
			bit with an 8 bit 1 so that we can get rid of all the other bits that don't contain any information about
			whether a robot is present, and then depending on this value, we concatenate different strings.	At the end, we
			combine the strings we made about the direction the robot is moving (which we change into the coordinates) and the
			robot string to form one complete statement to print to the GUI.
		  <p>
                    <img class="img-fluid d-block mx-auto" src="img/lab3/Actualswitchstatements.JPG" alt="">
                  </p>
		<p>
			After getting the GUI running, changed our code so that we had a transmit script and a receive script. We knew that
			we would have to separate the two codes because including the receive code on the robot Arduino would simply be a waste
			of space, and we ended up freeing up about 15% of our dynamic memory on the robot by removing this code. The most important
			step in this step was to change the pipe setup for each of the arduinos. This is accomplished by changing the role of the
			script. After doing this, our code was working separately.
		</p>
		</p>
                  <p> <h6>3.  Simulating Your Robot </h6>
                    	In the end, we integrated our code with the radio code and got the robot to drive around the maze as can be seen
			at the bottom of the page. We accomplished this by creating separate functions for the radio transmit code and
			including this file in the general code. Then when we wanted to send information to the receiver, which we chose to do
			at intersections, we would call the radio transmit function and pass in the bits we wanted to send.
                  </p>
		<p>
<img class="img-fluid d-block mx-auto" src="img/lab3/Code1.JPG" alt="">
		</p>

		<p>
<img class="img-fluid d-block mx-auto" src="img/lab3/Code2.JPG" alt="">
		</p>

		<p>
<img class="img-fluid d-block mx-auto" src="img/lab3/Code3.JPG" alt="">
		</p>

		<p>
<img class="img-fluid d-block mx-auto" src="img/lab3/Code4.JPG" alt="">
		</p>

		<p>
<img class="img-fluid d-block mx-auto" src="img/lab3/Code5.JPG" alt="">
		</p>



		<p>
The part of our code regarding turning is not included here for the sake of brevity.
		</p>

		<p>
<img class="img-fluid d-block mx-auto" src="img/lab3/Code9.JPG" alt="">
		</p>

		<p>
<img class="img-fluid d-block mx-auto" src="img/lab3/Code10.JPG" alt="">
		</p>

		<p>
<img class="img-fluid d-block mx-auto" src="img/lab3/Code11.JPG" alt="">
		</p>


<p>
	Above is some of our integrated code. The radio code is included at the top so we only have to call the radiosetup and ping_out
	functions within our code to send information to the ground station.
</p>

                  <p> <h5>Robot Team</h5>

                  <p> <h6>Objective</h6>
                   Integrate all the bits and pieces from labs 1 and 2, and milestone 1 and 2 on your robot.
                   Demonstrate that the robot starts on a 660Hz tone, does line following, and avoid walls.
                   It should also stop if its path is blocked by another robot, and continue if it is just a decoy.
                  </p>
                  <p> <h6>Robot Team Materials</h6>
                      <ul class="list-inline">
                      <li> Your robot, all the code from the past labs</li>
                      <li> Decoy </li>
                      <li> 660Hz tone generator </li>
                      <li> IR Hat </li>
                      <li> Microphone </li>
                      <li> Walls to make up following maze setup </li>
                    </ul>
                  </p>
                  <p>
                    <img class="img-fluid d-block mx-auto" src="img/lab3/image2.png" alt="">
                  </p>
                  <p> <h6>State Machine</h6>
                    Before we started integrating all of our code, we drew a FSM to decide how we wanted to integrate our code.
                    <img class="img-fluid d-block mx-auto" src="img/lab3/image3.png" alt="">
                  </p>
                  <p> <h6> Implementing a mux </h6>
                    We had run out of analog pins so before we could integrate the microphone, we needed to free up these pins.
                    To do so, we chose to mux the wall sensor inputs because we only check those at intersections.
                    If we chose to mux the light sensors then we would have risked the robot’s line following ability.
                    We connected our wall sensors to the mux as shown below:
                    <img class="img-fluid d-block mx-auto" src="img/lab3/image7.jpg" alt="">
                    To detect each wall, we placed them in different functions and updated the corresponding global variable.
                    One of the functions are shown below.
                    <p class= "text-left">
                      <code>
                        void detectLeftWall() { <br>
                          digitalWrite(s2, LOW); <br>
                          digitalWrite(s1, HIGH); <br>
                          digitalWrite(s0, LOW); <br>

                          read_wallL = analogRead(walls); <br>
                        }<br>
                      </code>
                    </p>

                    We called these functions at an intersection because that is the only time that we need to
                    detect walls and the way we were able to call the functions was simply:

                    <p class= "text-left">
                      <code>
                        detectFrontWall(); <br>
                        detectLeftWall(); <br>
                        detectRightWall();<br>
                      </code>
                    </p>
                  </p>
                  <p> <h6>Integrating the Microphone </h6>
                    In lab 2, the microphone code that we made utilized the ADCs free-running mode. However, because
                    the signal that we are trying to detect is a fairly low frequency, this is unnecessary. We also
                    don't want to sample too quickly because then the range of signals detected in a single bin would
                    be too large and make it difficult for us to distinguish 660Hz from similar frequencies.
                    We also had to take into account that our FFT code now has a sampling rate of 128 samples.
                    To account for this, we needed to test for the bin number and threshold again - using <code>AnalogRead</code>
                    and 128 samples.

                    <img class="img-fluid d-block mx-auto" src="img/lab3/image4.png" alt="">

                    We see that the bin number is 10 and the threshold is around 50,
                    but to be safe we drop the threshold in our detection code to be 40.

                    We run our 660Hz detection in our setup function so that it occurs prior to running anything else.
                    Doing it in this function also means that it won't constantly be checked during our loop.

                    <p class= "text-left">
                      <code>
                        int mic = 0; <br>
                        while (mic == 0) { <br>
                          mic = detectMicrophone(); <br>
                          Serial.println("Waiting for mic"); <br>
                        }<br>
                      </code>
                    </p>

                    The function <code> detectMicrophone </code> runs the FFT code using <code> AnalogRead </code>
                    to read the microphone for a signal and returns 1 if the bin we check is above the threshold.
                  </p>
                  <p> <h5> Full System </h5>
                    <video width = "500" height = "300" controls>
                       <source src = "img/lab3/Lab3.mp4" type = "video/mp4">
                    </video>
                    <br>

                    We can also show that we update the GUI as the robot drives through the maze.
                    <img src="img/lab3/image5.gif" alt="" height = "150" width = "250">
                    <img src="img/lab3/image6.gif" alt="" height = "150" width = "250">
                  </p>
		<p> <h5> Conclusion </h5>
			Our robot is now capable of starting on a 660 Hz tone, navigate a smaller test maze, and update the GUI. The robot still detects other robots based on their IR hat’s emissions and ignores the IR decoy. Right now our robot simply stops for 10 seconds when it detects a robot, and we may want to consider changing this action for the final competition.
		</p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 4 -->
    <div class="portfolio-modal modal fade" id="labsModal4" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
			<h2 class="text-uppercase">Lab 4: FPGA and Shape Detection</h2>
			<p class="item-intro text-muted"> Objective: In this lab, we are developing an FPGA module capable of detecting basic shapes from a camera input, and passing this information on to the Arduino. This device will be mounted on the robot to identify these shapes on the walls of the maze.
		  </p>
			<h5>Teams:</h5>
                    <ol class="list-inline">
                      <li>Elliot Sotnick and Jaylen Keith</li>
                      <li>April Chen and Brian Dempsey</li>
                    </ol>
			<h5>FPGA Team</h5>
			<p> <h6> Objective: Writing data into provided Dual_Port_M9K RAM, reading data it to the VGA display, and detecting the color of the bits in the RAM.</h6> </p>
			<p> <h6>FPGA Team Materials</h6>
                      <ul class="list-inline">
                      <li> DE0 Nano</li>
		      <li> VGA Adapter </li>
                    </ul>
                  </p>
		      <p> <h6>Procedure</h6>
		      <ul class="list-inline">
                      <li> 1. Setup</li>
	       	      <li> 2. Buffer Reader</li>
                    </ul>
		</p>
		   <p> <h6>1. Setup</h6>
                    We set up the PLL according to the lab and created wires to deliver the clock signals to the proper places.
                  </p>
		<p> <h6>2. Buffer Reader</h6>
		After connecting the VGA Driver to the monitor, we checked that we properly saw the black screen in the top left corner. We now wanted to get the M9K block working with the VGA Driver. To do this, we had to connect the M9K module and VGA module to the correct clocks from our PLL. To show that we could read and write color for each pixel, we created a pattern to display. This was done by incrementing the x and y address at each clock cycle and then changing the value we assigned to pixel_data_RGB332 depending on the coordinates. We were able to make the following picture of a cross.
		</p>
	      <img class="img-fluid d-block mx-auto" src="img/lab4/IMG_1933.JPG" alt="">
	      <p> Figure 1: Hard-coded Cross </p>
	      <img class="img-fluid d-block mx-auto" src="img/lab4/unnamed.png" alt="">
	      <p> Figure 2: M9k and VGA Modules </p>
	      <p> <h6>3. Downsampler</h6>
	      After achieving this, we moved on to the Downsampler. For this, we had to set up a bunch of GPIO pins for the camera to use. Except for the 24 MHz clock signal that we outputted to the camera, the reset were inputs from the camera. Next, we set up an always block that updated at PCLK, the camera’s clock. Within the always block, we checked to see if VSYNC was high or HREF was low (VSYNC going high indicates the end of valid data in a frame, and HREF going low indicates the end of the data for a row) and in these cases, we set the write enable to 0, reset the x coordinate to 0, set which_byte so that we would start with the first byte of color information the next time we wrote, and set pixel data to 0 so that we could control which color we were writing. If neither of those conditions were true, we checked the value of which_byte to see which colors we were receiving, and depending on the value either set the first 2 (least significant) bits with our blue information or we set the most significant 6 bits with our red and green information. We used this method because the camera sends 2 bytes with 5 bits for both red and blue, and 6 bits for green. The above method changes this data to a 332 format that the M9K block can use. At the end of the second byte, we set write enable high again so that the M9K block can write the information over to the VGA module. Below is the code for our downsampler.
	      </p>
	    <img class="img-fluid d-block mx-auto" src="img/lab4/downsamplercode.png" alt="">
	    <p> Figure 3: Downsampler code </p>
	    <p> <h6> 4. Color Detection </h6>
	    For color detection, we used an always block that runs when the clock goes high or VGA_VSYNC_NEG goes low. Within the always block, we determine whether each pixel has a majority red or blue content, and depending on the result, either increment the red or blue counter. Then after we finish a frame, we determine whether the values of the blue and red counters are above a certain threshold, and whether the blue or red counter has a greater value. Depending on the results, we then determine the value to send to the arduino (indicating red, blue, or no color). Below is the code we used.
	    </p>
		<img class="img-fluid d-block mx-auto" src="img/lab4/colordetectioncode.png" alt="">
	      <h5>Arduino Team</h5>
	      <p> <h6> Objective: Establish communication between camera and Arduino so camera registers can be read and written</h6> </p>
			<p> <h6>Arduino Team Materials</h6>
                      <ul class="list-inline">
		      <li> Arduino Uno </li>
                      <li> OV7670 Camera</li>
		      <li> 2 Pull up resistors</li>
		      <li> 24 MHz clock (from FPGA) </li>
                    </ul>
                  </p>
		<p> <h5>Procedure:</h5> After setting up the clock on the FPGA and wiring the I2C connection, we tested for any communication between the camera and Arduino. This is the schematic we used to wire the camera and Arduino together. </p>
		<img class="img-fluid d-block mx-auto" src="img/lab4/schematic.png" alt="">
<p>
There was no communication occurring between the devices, so we checked both the FPGA and Arduino. Originally, we thought that the distorted clock signal (more sinusoidal than square) coming from the FPGA was causing the issue, but after some testing the issue was found to be the program stopping after calling Wire.endTransmission(). Then, after some googling, we switched out our knock-off Arduino for a real one, and the two immediately were able to communicate. We then compared register values before and after writing them to verify that they we could change them.</p>
		<p> <h5>Integration</h5>
		Below is the picture of the color bar test after integrating the arduino and FPGA. </p>
		<img class="img-fluid d-block mx-auto" src="img/lab4/colortest.JPG" alt="">
		<p>The color bars somewhat correct.
Here is the video output:
		</p>
		<img class="img-fluid d-block mx-auto" src="img/lab4/videooutput.PNG" alt="">
		<p> <h5> Conclusion </h5>
			We were able to set up communication between the camera and Arduino. However, our color bars were not exactly correct, so we will need to rework how we are setting the registers to set them correctly so treasure colors can be detected. We will also need to decide how we want to implement the actual treasure detection for the competition, so we can correctly detect different shapes.
		</p>
		<button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
		</div>
	    	</div>
		</div>
		</div>
	</div>
	</div>
	</div>

    <!-- Milestone Modals -->

    <!-- Modal 1 -->
    <div class="portfolio-modal modal fade" id="portfolioModal1" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h2 class="text-uppercase">Milestone 1</h2>
                  <p class="item-intro text-muted">Objective: The objective of this milestone was to add line sensors to our robot to make it drive and follow a line, and to traverse a grid in a figure 8.</p>
                  <img class="img-fluid d-block mx-auto" src="img/milestone1/robot_full.jpg" alt="">
                  <p> <h5> Materials</h5>
                    <ul class="list-inline">
                      <li> 2 line sensors </li>
                      <li> Assorted wires</li>
                      <li> Stands for line sensors</li>
                      <li> Previous assembled robot</li>
                    </ul>
                  </p>
                  <p> <h5> Following a Line </h5>
                    <img class="img-fluid d-block mx-auto" src="img/milestone1/robot_light_sensors.jpg" alt="">
                    <p> <h6> Take 1 </h6>
                      Initially we added three line sensors the the front out our robot.
                      The middle one was set up to be over the line, and the two on the side were over either side of the line.
                      Our middle sensor would not read consistent values for the line,
                      and there was a difference in the values it read based on which part of the line the sensor was over,
                      and whether it was positioned vertically or horizontally over the line.
                      We attempted to used other robot parts to block light from reaching the sensor and interfering but the sensor still did not read the line properly.
                      We also did basic debugging strategies like trying out different pins and changing the orientation of the sensor.
                    </p>
                    <p> <h6> Take 2 </h6>
                      We decided on using just two line sensors on the front of the robot.
                      They are attached to stands under the robot so they all hang at the same height above the ground.
                      They are spaced so that there is one sensor on either side of the line.
                      We coded our robot to drive straight as long as both sensors read the voltage of the tile.
                      When the right sensor read the voltage of the line and the robot was off track, we coded the right servo to stop and the left servo to continue straight until both sensors read the voltage of the tile again,
                      to adjust the robot back onto the line.
                      The same process was done for when the left servo read the voltage of the line.
                      Here is the main section of the code for this, which was implemented in a loop.

                      <p class = "text-left">
                        <code> if (readR >= 800 && readL >= 800) { <br>
  	                       leftservo.write(135); <br>
  	                        rightservo.write(45); <br>
                          } <br>
                          else if (readR < 800 && readL >= 800) { <br>
  	                         leftservo.write(135); <br>
  	                          rightservo.write(90); <br>
                            } <br>
                            else if (readR >= 800 && readL < 800) { <br>
  	                           leftservo.write(90); <br>
  	                            rightservo.write(45); <br>
                              }
                        </code>
                      </p>
                    </p>
                  </p>
                  <p> <h5> Doing a Figure Eight </h5>
                    For the figure eight the robot is coded to drive straight and call our turn function to make the turns of
                    the figure eight when both sensors sense the voltage of the line.
                    This happens when the robot is at an intersection. First the robot makes 4 left turns,
                    then 4 right turns, and continues this cycle. Here is the code for our function to turn
                    the robot in the sequence of a figure eight.

                    <p class = "text-left">
                      <code> void turn() { <br>
	                       if (turnCount % 8 < 4) {  //left turn <br>
		                         turnCount++; <br>
		                         leftservo.write(90); <br>
		                         rightservo.write(45); <br>
		                         delay(1200); <br>
	                       } <br>
	                       else {  //right turn <br>
		                         turnCount++; <br>
		                         leftservo.write(135); <br>
		                         rightservo.right(90); <br>
		                         delay(1200); <br>
	                        } <br>
                      }<br>
                      </code>
                    </p>
                  </p>
                  <p>
                    <img class="img-fluid d-block mx-auto" src="img/milestone1/Figure1.png" alt="">
                    <u> Figure 1: Block Diagram </u>
                    <p>
                      Figure 1 shows a high level circuit diagram of our current robot setup.
                      As can be seen, the light sensors are attached to two of the analog pins on the arduino
                      and the two servos are attached to two PWM pins.
                      The light sensors are connected directly to the arduino pins because their wires are not
                      long enough to reach the breadboard. The servos, which have much longer wires,
                      are connected through our breadboard to help organize the wiring on the robot,
                      ensuring the wires do not end up in the wheels of the robot.
                    </p>
                  </p>
                  <p>
                    <img class="img-fluid d-block mx-auto" src="img/milestone1/Figure2.jpg" alt="">
                    <u> Figure 2: Breadboard on the Robot </u>
                    <p>
                      On the breadboard, we have the power and ground rails coming from the arduino 5V and GND pins.
                      The light sensors and the servos get their power through these rails
                      (the servo connections are in the bottom left corner and the light sensors are connected on the right side of Figure 2).
                    </p>
                  </p>
                  <p> <h5> Conclusion </h5>
                    Our robot was successful in following a line and a figure eight.
                    From here on it will be important to not rely on timing with the delay function
                    to complete turns and to instead rely on the line sensors,
                    but timing is effective for this type of task.
                  </p>
                  <p>
                    <div class="embed-responsive embed-responsive-16by9">
                      <iframe width="560" height="315" src="https://www.youtube.com/embed/kkk0G6GhV58" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                  </p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 2 -->
    <div class="portfolio-modal modal fade" id="portfolioModal2" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h2 class="text-uppercase">Milestone 2</h2>
                  <p class="item-intro text-muted">Objective: The objective of this milestone was to update our robot to circle a set of arbitrary walls through right-hand wall following and successfully avoid other robots. </p>
                  <img class="img-fluid d-block mx-auto" src="img/milestone2/image1.PNG" alt="">
                  <p> <h5> Materials</h5>
                    <ul class="list-inline">
                      <li> 3 Wall Sensors </li>
                      <li> Robot</li>
                    </ul>
                  </p>
                  <p> <h5> Wall Following</h5>
                    Our robot traverses the maze by using right-hand wall following,
                    which means it always turns right at intersections unless it senses a wall to its right.
                    To code our robot to do this, first we found values for the walls on the left,
                    right and front and set them as global variables so we could compare the readings from the sensors,
                    and decide which way to turn. The right sensor is connected to pin A3, the left is connected to A4,
                    and the front sensor is connected to pin A5 on the Arduino. Below are the global variables for the walls,
                    with LRwalls being for the walls on the left and right, and Fwall being for a wall in front of the robot.

                    <p class = "text-left">
                      <code>
                        int LRwalls = 195; <br>
                        int Fwall = 100; <br>
                      </code>
                    </p>

                    And here is the code for deciding which way to turn based on the readings from the sensors.
                    If there are walls on the front, right and left, the robot does a U-turn.
                    If there is a right wall, a front wall, and no left wall, the robot turns left.
                    If there is no right wall, the robot turns right.
                    The turn function call uses the number in its argument to decide which way to turn the robot.
                    When the robot is not turning, it uses the line following code from previous labs to go straight.

                    <p class = "text-left">
                      <code>
                        // U-turn <br>
                        if (read_wallF >= Fwall && read_wallL >= LRwalls && read_wallR >= LRwalls) { <br>
                          turn(2); <br>
                        }<br>
                        // Left Turn <br>
                        else if (read_wallF >= Fwall && read_wallL < LRwalls && read_wallR >= LRwalls) { <br>
                          turn(0); <br>
                        } <br>
                        // Right Turn <br>
                        else if (read_wallR < LRwalls) { <br>
                          turn(1); <br>
                        } <br>
                        // Go forward <br>
                        else { <br>
                        leftservo.write(135); <br>
                        rightservo.write(45); <br>
                      } <br>
                      </code>
                    </p>
                  </p>
                  <p> <h5> Robot Detection </h5>
                    Our robot currently only checks for other robots at intersections where we call our detectRobot() function.
                    We chose to do this because when we are at an intersection,
                    we can choose to go a different direction should we detect a robot instead of stopping.
                    However, for this milestone, we simply flash an external LED to show that the signal has been detected.
                    Below is the code for robot detection at an intersection.

                    <p class = "text-left">
                      <code>
                        robot = detectRobot(); <br>
                        if (robot == 1) { <br>
                          digitalWrite(7, HIGH); <br>
                          Serial.println("ROBOT"); <br>
                          leftservo.write(90); <br>
                          rightservo.write(90); <br>
                          delay(1000); <br>
                          digitalWrite(7, LOW); <br>
                        } <br>
                        else { <br>
                          Serial.println("no robot"); <br>
                        }<br>
                      </code>
                    </p>

                    detectRobot() used the FFT code from lab 2 and checks the bin containing the 6.08 kHz frequency to see if it is above a threshold of 70.
                    To decrease the amount of dynamic memory our robot used,
                    we lowered the sampling rate from 256 samples to 128 samples after reading the FTT library’s ReadMe file. <br>

                    <img class="img-fluid d-block mx-auto" src="img/milestone2/image2.png" alt="">

                    Because the FFT and Servo interfere with each other,
                    we saved the default values that are changed when using the Analog to Digital Converter
                    free running mode and reset them after prior to exiting <code>detectRobot()</code>. The code for robot detection is below
                    with FFT code from lab 2 omitted):

                    <p class = "text-left">
                      <code>
                        int detectRobot() { <br>
                        //default adc values <br>
                        unsigned int default_timsk = TIMSK0; <br>
                        unsigned int default_adcsra = ADCSRA; <br>
                        unsigned int default_admux = ADMUX; <br>
                        unsigned int default_didr = DIDR0; <br>

                        //setup <br>
                        TIMSK0 = 0; // turn off timer0 for lower jitter <br>
                        ADCSRA = 0xe5; // set the adc to free running mode <br>
                        ADMUX = 0x40; // use adc0 <br>
                        DIDR0 = 0x01; // turn off the digital input for adc0 <br>

                        Runing FFT code from lab 2 <br>

                        //checking the bin <br>
                        if (fft_log_out[23] >= 70) { <br>
                          TIMSK0 = default_timsk; <br>
                          ADCSRA = default_adcsra; <br>
                          ADMUX = default_admux; <br>
                          DIDR0 = default_didr; <br>
                          return 1; <br>
                        } <br>
                        else { <br>
                          TIMSK0 = default_timsk; <br>
                          ADCSRA = default_adcsra; <br>
                          ADMUX = default_admux; <br>
                          DIDR0 = default_didr; <br>
                          return 0; <br>
                        } <br>
                      }<br>
                      </code>
                    </p>
                  </p>
                  <p> <h5> Video Demonstration </h5>
                    <div class="embed-responsive embed-responsive-16by9">
                      <iframe width="560" height="315" src="https://www.youtube.com/embed/xU6shaumFss" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
                    </div>
                  </p>
		  <p> <h5> Conclusion </h5>
		  We were able to get out robot to use righthand wall following to traverse the maze. Our issue with the FTT taking up too much memory was resolved by reducing the number of samples we had to it take. For robot detection, the robot can detect when another robot is nearby, but as of now only prints to the serial monitor when it does. For the competition, we will need to update our robot to stop instead of just printing something when a robot is detected. We could further improve how our robot traverses the maze by using efficient search methods instead of just righthand wall following.
		  </p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 3 -->
    <div class="portfolio-modal modal fade" id="portfolioModal3" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
		  <h2 class="text-uppercase">Milestone 3</h2>
		<p class="item-intro text-muted">Objective: Create an algorithm that allows the robot to traverse a maze and update the GUI.</p>
		  <p class="item-intro text-muted">Materials: Fully assembled robot capable of moving, detecting walls, and line following.</p>

      <p> <h5> DFS </h5>
        <p> <h6> Algorithm </h6>
        We used the Depth-first search algorithm to implement this milestone.
        Depth-first search is an algorithm for traversing the nodes of a graph.
        In abstract terms, the algorithm navigates a graph by traversing “deeper”
        in some way until there are no more possible paths or all possible paths have already been “visited.”
        When this happens, the algorithm then backtracks to the last point of the path with another unvisited path.
        This happens until the entire maze has been explored.
        </p>

        <p> <h6> Implementation </h6>
          To store data of where the robot has already traversed, we created a 2D array of nodes.
          These nodes were based off the struct below.
          <p class = text-left>
            <code>
              struct node { <br>
                bool visited; <br>
                maze_direction dir; <br>
              }; <br>
            </code>
          </p>
          visited is a boolean that tells if that node of the maze has been visited or not.
          dir is a maze_direction which is an enum that we created to send the direction the robot is going in
          to the gui. dir is the direction that the robot took to get to that node. This is necessary for the
          way we implemented the backtracking. We also keep track of the current coordinates and whether or not
          we are backtracking.

          <br>

          When the robot reaches an intersection, it must determine where to go next. In this determination process,
          we simply follow right-hand wall following if it is still possible. This means that if not all side (front, left, right)
          of the robot are blocked, then we simply wall follow. Being blocked means one of two things: there is a wall, or the
          node to that side has already been visited. We do this by setting these variables to true.

          <p class = text-left>
            <code>
              bool r_blocked <br>
              bool l_blocked <br>
              bool f_blocked<br>
            </code>
          </p>

          For us to be able to easily determine the coordinates of the surrounding nodes to check,
          we must calculate them before hand. We can do this because our directions are enums so we
          can calculate the right and left directions with some simple algebra.

          <p class = text-left>
            <code>
              int right = (m_direction + 1) % 4; <br>
              int left = (m_direction + 3) % 4;<br>
            </code>
          </p>

          This allows us to then pass this direction into methods that calculate coordinates.

          <img class="img-fluid d-block mx-auto" src="img/milestone3/img1.png" alt="">

          If the robot must backtrack, it sets the <code> backtracking </code> variable to
          <code> true </code> and then proceeds to calculate how to move. The normal wall-following
          always sets this variable back to <code> false </code> so that the robot won't always
          think it's backtracking. To calculate how to backtrack, we use <code> dir </code>.
          <code> dir </code> has told us how we originally got to that node. We also make sure not
          to update <code> dir </code> if we are backtracking which is why that variable is necessary. We can then
          calculate what that direction is using similar algebra to before and then calculate the coordinates as above as well.
          We then are able to determine how to move.

          <br>

          After we turn, we make sure to update <code> m_direction </code> to be able to properly update the gui.
          We also update the maze every time we hit an intersection.

        </p>

        <p> <h5> VIDEO </h5>
		    </p>

          <iframe width="560" height="315" src="https://www.youtube.com/embed/saag1yeGhzc" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

          <iframe width="560" height="315" src="https://www.youtube.com/embed/7omZUuilMRM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

		  <p>
          This shows that our robot is functioning. However, there is still a bug where sometimes
          it knows to turn but yet doesn't complete the turn which can be seen at a point in the video
          where April sets the robot back on the correct path. Additionally, if one looks closely at the mapped
	    maze, there is one wall that the robot 'saw' that was not present for the big maze, and the robot missed
	the three walls that made it necessary for the robot to turn around in the small maze. However, the rest of the maze
	    was spot on.
        </p>


      </p>
		<p> <h5> Conclusion </h5>
		Our depth first search algorithm generally works, but there are some bugs remaining with the robot being able to detect some walls and being able to complete turns every time. We need to make our wall detection and turning more consistent in the future to improve the traversal, but the robot showed that it is able to function properly.
		</p>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 4 -->
    <div class="portfolio-modal modal fade" id="portfolioModal4" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
		<h2 class="text-uppercase">Milestone 4</h2>
                  <p class="item-intro text-muted">Objective: Robot will be capable of detecting whether treasures are present, what color the treasures are, and what shape the treasures are.</p>
		  <p class="item-intro text-muted">Materials: FPGA, Arduino Uno, various wires, VGA adapter, VGA cables, monitor</p>
                  <p>The heart of this milestone is our image processor code. In this code, we determine the shape of the object when VGA_VSYNC_NEG goes low, indicating the end of a frame. At this point, we don’t need to read the incoming data and can quickly go through and figure out what the previous image was. We are able to figure out the color by comparing the number of blue pixels detected with the number of red pixels detected. Additionally, we make sure that the number of pixels detected is greater than a threshold, indicating we are probably seeing a treasure and not just a higher concentration of red or blue pixels. After doing this, we determine the shape of the object by comparing the concentration of pixels at the top, middle, and bottom of the image. We know that for a square all three concentrations should be equal, for a triangle the bottom should be greater than the middle which should be greater than the top, and for the diamond, the bottom should be less than the middle which is greater than the top. Depending on what we find, we determine the value of result which is a parallel connection to the arduino and tells the arduino which shape and color we have detected. </p>
                  <p> Below is our image detection code: </p>

			 <img class="img-fluid d-block mx-auto" src="img/milestone4/Code3.JPG" alt="">
			 <img class="img-fluid d-block mx-auto" src="img/milestone4/Code4.JPG" alt="">
			 <img class="img-fluid d-block mx-auto" src="img/milestone4/Code5.JPG" alt="">

		  <p> We spent about 2 weeks trying to use our own code to run the camera and still had no luck. At this point we turned to Group 5’s code because we knew they had it working for their camera. After changing all the IO pins so that it should have worked for our setup, it still didn’t work at all. After another week, we found a single wire with poor connection to the breadboard. We were so far behind at this point that we simply continued working with Group 5’s code since it was nearly identical to our own but we knew it would function properly. </p>
                  <p>
			  Below is a picture of our shape detection code working. In each picture is the view from the camera and a view of the serial monitor displaying information from the arduino telling the user what shape and color object it is looking at.
			</p>
			<img class="img-fluid d-block mx-auto" src="img/milestone4/Combined.JPG" alt="">
			<p> <h5> Conclusion </h5>
			After weeks of work our image detection and processing code works properly. When the camera sees the different treasures, it is able to correctly print out the color and shape of the treasure.
			</p>
			<button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 5 -->
    <div class="portfolio-modal modal fade" id="portfolioModal5" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h2 class="text-uppercase">Project Name</h2>
                  <p class="item-intro text-muted">Lorem ipsum dolor sit amet consectetur.</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/05-full.jpg" alt="">
                  <p>Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!</p>
                  <ul class="list-inline">
                    <li>Date: January 2017</li>
                    <li>Client: Southwest</li>
                    <li>Category: Website Design</li>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Modal 6 -->
    <div class="portfolio-modal modal fade" id="portfolioModal6" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h2 class="text-uppercase">Project Name</h2>
                  <p class="item-intro text-muted">Lorem ipsum dolor sit amet consectetur.</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/06-full.jpg" alt="">
                  <p>Use this area to describe your project. Lorem ipsum dolor sit amet, consectetur adipisicing elit. Est blanditiis dolorem culpa incidunt minus dignissimos deserunt repellat aperiam quasi sunt officia expedita beatae cupiditate, maiores repudiandae, nostrum, reiciendis facere nemo!</p>
                  <ul class="list-inline">
                    <li>Date: January 2017</li>
                    <li>Client: Window</li>
                    <li>Category: Photography</li>
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

<!-- Modal 7 -->
    <div class="portfolio-modal modal fade" id="finalproj" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
			<h2 class="text-uppercase">Final Robot Design</h2>
			<p class="item-intro text-muted"> Here we highlight the design decisions we made for our final design and explain why we made these decisions.
		  </p>
			<h5>Description of Robot</h5>
			<p> 
			Our final robot consisted of:
				<ul class="list-inline">
					<li> 1 Battery </li>
					<li> 2 Servos </li>
					<li> 1 Arduino Uno </li>
					<li> 1 Small Breadboard </li>
					<li> 3 Wall Sensors </li>
					<li> 2 Analog Line Sensors </li>
					<li> 1 Microphone </li>
					<li> 1 Photodetector </li>
					<li> 2 Op Amps </li>
					<li> Various Passive Components </li>
					<li> Various Mechanical Components from Lab </li>
				</ul>
			</p>
		      <p>
			      The robot has three levels, and the components are divided between them as shown below:
		      </p>
		      <img class="img-fluid d-block mx-auto" src="img/final/final_layer_diagram.JPG" alt="">
		      
		      <h5> Functionality: </h5>
		      <p>
			  <ul class="list-inline">
				  <li> Recognize 660 Hz audio tone </li>	
				  <li> Follow maze lines, recognize intersections, and detect walls </li>
				  <li> Detect nearby robots </li>
				  <li> Transmit data through radio transmitter </li>
				  <li> Map maze during traversal </li>
				  <li> Traverse maze efficiently through depth first search-style algorithm </li>
		      	  </ul>
		      </p>
		    <h5> Why we did what we did: </h5>
		    <p>
			    Packaging and Mechanical Design
		    </p>
		    <p>
			    After several iterations of our robot’s packaging, we settled on a three-level design in which each level has a distinct purpose. The bottom level contains the battery and is a mounting point for the line sensors. Besides powering the robot, the battery helps maintain a low center of gravity and therefore stability (like in a Tesla). This fixed a previous issue where the robot would often tip over when it started moving. Mounting the line sensor on the bottom level also keeps them low to the ground and as accurate as possible, increasing turning and line following reliability.
				The second level contains the Arduino, a power bus, and the radio transmitter. The power bus is a two-column breadboard, with one column connected to the Arduino 5V and the other connected to Arduino ground. This allows us to have easy access to our power supply and easily diagnose wiring issues. Having the Arduino and radio protected between the top and middle levels also reduces the likelihood of wiring failures or accidental shorts to the Arduino. The wall sensors are also mounted on this level to be near the middle of the wall and out of the way of any wires that could obstruct them.
				The third level contains a small breadboard containing all of our lab-made circuits. The placement of this breadboard allows for easy access to the circuits, making debugging quick and easy. Wires to the power bus or Arduino can be easily routed through or around the top level’s “floor”.
				The servos are mounted to the middle level with standard sized wheels.
		    </p>
		    <p>
			    Hardware:
		    </p>
		    <p>
			    All of the robot’s custom-made and exciting electronics are contained on the breadboard on the top level. A picture of the breadboard and schematic of the two op amp circuits are shown below.
		    </p>
		    
			<p> <h6>FPGA Team Materials</h6>
                      <ul class="list-inline">
                      <li> DE0 Nano</li>
		      <li> VGA Adapter </li>
                    </ul>
                  </p>
		      <p> <h6>Procedure</h6>
		      <ul class="list-inline">
                      <li> 1. Setup</li>
	       	      <li> 2. Buffer Reader</li>
                    </ul>
		</p>
		   <p> <h6>1. Setup</h6>
                    We set up the PLL according to the lab and created wires to deliver the clock signals to the proper places.
                  </p>
		<p> <h6>2. Buffer Reader</h6>
		After connecting the VGA Driver to the monitor, we checked that we properly saw the black screen in the top left corner. We now wanted to get the M9K block working with the VGA Driver. To do this, we had to connect the M9K module and VGA module to the correct clocks from our PLL. To show that we could read and write color for each pixel, we created a pattern to display. This was done by incrementing the x and y address at each clock cycle and then changing the value we assigned to pixel_data_RGB332 depending on the coordinates. We were able to make the following picture of a cross.
		</p>
	      <img class="img-fluid d-block mx-auto" src="img/lab4/IMG_1933.JPG" alt="">
	      <p> Figure 1: Hard-coded Cross </p>
	      <img class="img-fluid d-block mx-auto" src="img/lab4/unnamed.png" alt="">
	      <p> Figure 2: M9k and VGA Modules </p>
	      <p> <h6>3. Downsampler</h6>
	      After achieving this, we moved on to the Downsampler. For this, we had to set up a bunch of GPIO pins for the camera to use. Except for the 24 MHz clock signal that we outputted to the camera, the reset were inputs from the camera. Next, we set up an always block that updated at PCLK, the camera’s clock. Within the always block, we checked to see if VSYNC was high or HREF was low (VSYNC going high indicates the end of valid data in a frame, and HREF going low indicates the end of the data for a row) and in these cases, we set the write enable to 0, reset the x coordinate to 0, set which_byte so that we would start with the first byte of color information the next time we wrote, and set pixel data to 0 so that we could control which color we were writing. If neither of those conditions were true, we checked the value of which_byte to see which colors we were receiving, and depending on the value either set the first 2 (least significant) bits with our blue information or we set the most significant 6 bits with our red and green information. We used this method because the camera sends 2 bytes with 5 bits for both red and blue, and 6 bits for green. The above method changes this data to a 332 format that the M9K block can use. At the end of the second byte, we set write enable high again so that the M9K block can write the information over to the VGA module. Below is the code for our downsampler.
	      </p>
	    <img class="img-fluid d-block mx-auto" src="img/lab4/downsamplercode.png" alt="">
	    <p> Figure 3: Downsampler code </p>
	    <p> <h6> 4. Color Detection </h6>
	    For color detection, we used an always block that runs when the clock goes high or VGA_VSYNC_NEG goes low. Within the always block, we determine whether each pixel has a majority red or blue content, and depending on the result, either increment the red or blue counter. Then after we finish a frame, we determine whether the values of the blue and red counters are above a certain threshold, and whether the blue or red counter has a greater value. Depending on the results, we then determine the value to send to the arduino (indicating red, blue, or no color). Below is the code we used.
	    </p>
		<img class="img-fluid d-block mx-auto" src="img/lab4/colordetectioncode.png" alt="">
	      <h5>Arduino Team</h5>
	      <p> <h6> Objective: Establish communication between camera and Arduino so camera registers can be read and written</h6> </p>
			<p> <h6>Arduino Team Materials</h6>
                      <ul class="list-inline">
		      <li> Arduino Uno </li>
                      <li> OV7670 Camera</li>
		      <li> 2 Pull up resistors</li>
		      <li> 24 MHz clock (from FPGA) </li>
                    </ul>
                  </p>
		<p> <h5>Procedure:</h5> After setting up the clock on the FPGA and wiring the I2C connection, we tested for any communication between the camera and Arduino. This is the schematic we used to wire the camera and Arduino together. </p>
		<img class="img-fluid d-block mx-auto" src="img/lab4/schematic.png" alt="">
<p>
There was no communication occurring between the devices, so we checked both the FPGA and Arduino. Originally, we thought that the distorted clock signal (more sinusoidal than square) coming from the FPGA was causing the issue, but after some testing the issue was found to be the program stopping after calling Wire.endTransmission(). Then, after some googling, we switched out our knock-off Arduino for a real one, and the two immediately were able to communicate. We then compared register values before and after writing them to verify that they we could change them.</p>
		<p> <h5>Integration</h5>
		Below is the picture of the color bar test after integrating the arduino and FPGA. </p>
		<img class="img-fluid d-block mx-auto" src="img/lab4/colortest.JPG" alt="">
		<p>The color bars somewhat correct.
Here is the video output:
		</p>
		<img class="img-fluid d-block mx-auto" src="img/lab4/videooutput.PNG" alt="">
		<p> <h5> Conclusion </h5>
			We were able to set up communication between the camera and Arduino. However, our color bars were not exactly correct, so we will need to rework how we are setting the registers to set them correctly so treasure colors can be detected. We will also need to decide how we want to implement the actual treasure detection for the competition, so we can correctly detect different shapes.
		</p>
		<button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
		</div>
	    	</div>
		</div>
		</div>
	</div>
	</div>
	</div>

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Contact form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/agency.min.js"></script>

  </body>

</html>
